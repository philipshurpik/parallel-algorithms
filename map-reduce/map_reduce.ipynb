{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "N = 25\n",
    "WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, max_value=9):\n",
    "    poisson_data = np.random.poisson(lam=1, size=2**n)\n",
    "    poisson_data = poisson_data[poisson_data <= max_value]\n",
    "    remaining = 2**n - len(poisson_data)\n",
    "    remaining_data = np.random.randint(0, max_value, remaining, dtype='h')\n",
    "    return np.concatenate((poisson_data, remaining_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate and check data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = generate_data(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_by_element(elements):\n",
    "    key_vals = {}\n",
    "    for key in elements:\n",
    "        key_vals[key] = key_vals.get(key, 0) + 1\n",
    "    return key_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 12339598,\n",
       " 1: 12342904,\n",
       " 2: 6174943,\n",
       " 3: 2058482,\n",
       " 4: 515237,\n",
       " 5: 103305,\n",
       " 6: 17199,\n",
       " 7: 2440,\n",
       " 8: 299,\n",
       " 9: 25}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit -r1 -n1 get_sum_by_element(elements)\n",
    "get_sum_by_element(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_by_element_quick(elements):\n",
    "    # print(\"Map process name: \", multiprocessing.current_process().name)\n",
    "    key_vals = {}\n",
    "    for i in np.unique(elements):\n",
    "        key_vals[i] = len(elements[elements == i])\n",
    "    return key_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15 s ± 0 ns per loop (mean ± std. dev. of 1 run, 4 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 12339598,\n",
       " 1: 12342904,\n",
       " 2: 6174943,\n",
       " 3: 2058482,\n",
       " 4: 515237,\n",
       " 5: 103305,\n",
       " 6: 17199,\n",
       " 7: 2440,\n",
       " 8: 299,\n",
       " 9: 25}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit -r1 -n4 get_sum_by_element_quick(elements)\n",
    "get_sum_by_element_quick(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick implementation of mapper works on average 4-7 quicker than naive one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do map reduce (for mapper using both functions get_sum_by_element from above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(map_results):\n",
    "    data = defaultdict(list)\n",
    "    for item in map_results:\n",
    "        for key, value in item.items():\n",
    "            data[key].append(value)\n",
    "    return list(data.items())\n",
    "\n",
    "def reduce_data(data):\n",
    "    # print(\"Reduce process name: \", multiprocessing.current_process().name, \"Key: \", data[0])\n",
    "    return (data[0], np.sum(data[1]))\n",
    "\n",
    "def run_map_reduce(pool, elements, mapper_function):\n",
    "    elements_parts = np.array_split(elements, WORKERS)\n",
    "    mapped = pool.map(mapper_function, elements_parts)\n",
    "    shuffled = shuffle_data(mapped)\n",
    "    results = pool.map(reduce_data, shuffled)\n",
    "    \n",
    "    return results, shuffled, mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = multiprocessing.Pool(WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.7 s ± 0 ns per loop (mean ± std. dev. of 1 run, 4 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n4 run_map_reduce(pool, elements, get_sum_by_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.37 s ± 0 ns per loop (mean ± std. dev. of 1 run, 4 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r1 -n4 run_map_reduce(pool, elements, get_sum_by_element_quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Map results:  [{0: 1542213, 1: 1543051, 2: 771820, 3: 257679, 4: 64410, 5: 12660, 6: 2143, 7: 289, 8: 37, 9: 2}, {0: 1542380, 1: 1542408, 2: 771152, 3: 258008, 4: 64733, 5: 13115, 6: 2167, 7: 287, 8: 46, 9: 8}, {0: 1543549, 1: 1542508, 2: 771081, 3: 257562, 4: 64149, 5: 12960, 6: 2126, 7: 331, 8: 37, 9: 1}, {0: 1541862, 1: 1544106, 2: 771794, 3: 256755, 4: 64297, 5: 12911, 6: 2231, 7: 299, 8: 42, 9: 7}, {0: 1543879, 1: 1542459, 2: 771325, 3: 256464, 4: 64588, 5: 13129, 6: 2143, 7: 285, 8: 29, 9: 3}, {0: 1542164, 1: 1543242, 2: 772097, 3: 257152, 4: 64366, 5: 12820, 6: 2095, 7: 332, 8: 33, 9: 3}, {0: 1541515, 1: 1543161, 2: 772305, 3: 257829, 4: 64202, 5: 12807, 6: 2151, 7: 294, 8: 39, 9: 1}, {0: 1542036, 1: 1541969, 2: 773369, 3: 257033, 4: 64492, 5: 12903, 6: 2143, 7: 323, 8: 36}]\n",
      "\n",
      "Shuffle results:  [(0, [1542213, 1542380, 1543549, 1541862, 1543879, 1542164, 1541515, 1542036]), (1, [1543051, 1542408, 1542508, 1544106, 1542459, 1543242, 1543161, 1541969]), (2, [771820, 771152, 771081, 771794, 771325, 772097, 772305, 773369]), (3, [257679, 258008, 257562, 256755, 256464, 257152, 257829, 257033]), (4, [64410, 64733, 64149, 64297, 64588, 64366, 64202, 64492]), (5, [12660, 13115, 12960, 12911, 13129, 12820, 12807, 12903]), (6, [2143, 2167, 2126, 2231, 2143, 2095, 2151, 2143]), (7, [289, 287, 331, 299, 285, 332, 294, 323]), (8, [37, 46, 37, 42, 29, 33, 39, 36]), (9, [2, 8, 1, 7, 3, 3, 1])]\n",
      "\n",
      "Reduce results:  [(0, 12339598), (1, 12342904), (2, 6174943), (3, 2058482), (4, 515237), (5, 103305), (6, 17199), (7, 2440), (8, 299), (9, 25)]\n"
     ]
    }
   ],
   "source": [
    "results, shuffled, mapped = run_map_reduce(pool, elements, get_sum_by_element_quick)\n",
    "print(\"\\nMap results: \", mapped)\n",
    "print(\"\\nShuffle results: \", shuffled)\n",
    "print(\"\\nReduce results: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Execution results (milliseconds):\n",
    "### N:               15       20       22       25\n",
    "# 1) slow mapper:   11.8     359      1047     11900\n",
    "# 2) quick mapper:  1.84     48.7     218      2150\n",
    "# 3) pool (slow):   7.45     117      493      7700\n",
    "# 4) pool (quick):  2.93     24.5     93       7370"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As conclusion:\n",
    "1) Quick mapper, that utilises numpy syntax works 4-7 times quicker than naive one  \n",
    "2) On small amount of data (2^15) just regular one thread mapper works quicker than multi processes (because no need for shuffle and cross threads communication)  \n",
    "3) On bigger amount of data (2^20 - 2^22) MapReduce versions works 2-3 times quicker than just single threaded mappers. On that amount we benefit from parallel maps  \n",
    "4) On much bigger data MapReduce versions work slower than just numpy mapper - reason is because shuffle operation is implemented in python in not optimal way  \n",
    "5) But still slow (python mapper) MapReduce version perform better than regular slow mapper function - because in that case both shuffle and map functions are implemented non optimally - so we get some benefits from multiple processes :)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
